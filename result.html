<html>
<head>
<title>ChatGLM & GLM</title>
</head>
<body>
<pre><font color="#4E9A06"><b>ubuntu@192-9-147-106</b></font>:<font color="#3465A4"><b>~/PycharmProjects/WuDao-GLM</b></font>$ python main.py
[INFO] ChatGLM
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:07&lt;00:00,  1.06it/s]
W0424 19:29:32.879323 139679895955264 logging.py:295] The dtype of attention mask (torch.int64) is not bool
2023-04-24 19:29:35.808285: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-24 19:29:35.874118: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-24 19:29:36.520157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
--------------------------------------------------------------------------
WARNING: No preset parameters were found for the device that Open MPI
detected:

  Local host:            192-9-147-106
  Device name:           mlx5_0
  Device vendor ID:      0x02c9
  Device vendor part ID: 4126

Default device parameters will be used, which may result in lower
performance.  You can edit any of the files specified by the
btl_openib_device_param_files MCA parameter to set values for your
device.

NOTE: You can turn off this warning by setting the MCA parameter
      btl_openib_warn_no_device_params_found to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           192-9-147-106
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       udcm
--------------------------------------------------------------------------
2023-04-24 19:29:37.155683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-04-24 19:29:37.157175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-04-24 19:29:37.157743: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0424 19:29:37.507889 139679895955264 utils.py:129] Note: NumExpr detected 30 cores but &quot;NUMEXPR_MAX_THREADS&quot; not set, so enforcing safe limit of 8.
I0424 19:29:37.508062 139679895955264 utils.py:141] NumExpr defaulting to 8 threads.
/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version &apos;2.7.3&apos; or newer of &apos;numexpr&apos; (version &apos;2.7.1&apos; currently installed).
  from pandas.core.computation.check import NUMEXPR_INSTALLED
你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。
晚上睡不着可能会让人感到困扰和不安，但有一些方法可以帮助你入睡：

1. 创造一个舒适的睡眠环境：确保房间安静、黑暗、凉爽和舒适。你也可以使用舒适的床垫和枕头，以及一个柔软的床单和被子。

2. 放松身体和头脑：使用深呼吸、渐进性肌肉松弛和其他放松技巧来减轻身体和头脑的压力。还可以试着进行冥想或瑜伽练习来放松身心。

3. 避免使用电子设备：避免在睡前使用电子设备，如手机、电脑和电视。这些设备发出的蓝光可能会干扰睡眠。

4. 制定固定的睡眠时间表：尽量在每天的相同时间上床睡觉，并制定固定的起床时间。这有助于身体建立健康的睡眠习惯。

5. 避免摄入咖啡因和酒精：咖啡因和酒精可能会影响睡眠，因此在晚上尽量避免摄入这些物质。

6. 尝试一些放松的活动：试着做一些轻松的事情，如阅读、听音乐或洗澡，来缓解压力和放松身心。

如果这些方法无法帮助你入睡，你可以考虑咨询医生或睡眠专家，获取更专业的建议和帮助。
[INFO] GLM
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input&apos;s `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50007 for open-end generation.
[CLS] 凯旋门位于意大利米兰市古城堡旁。1807年为纪念 [MASK] 而建,门高25米,顶上矗立两武士青铜古兵车铸像。 &lt;|endoftext|&gt; &lt;|startofpiece|&gt; 拿破仑军队攻克米兰城 &lt;|endofpiece|&gt;
<font color="#4E9A06"><b>ubuntu@192-9-147-106</b></font>:<font color="#3465A4"><b>~/PycharmProjects/WuDao-GLM</b></font>$

</pre>
</body>
</html>
