<html>
<head>
<title>ChatGLM & GLM</title>
</head>
<body>
<pre><font color="#4E9A06"><b>ubuntu@192-9-147-106</b></font>:<font color="#3465A4"><b>~/PycharmProjects/WuDao-GLM</b></font>$ python main.py
[INFO] ChatGLM
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:07&lt;00:00,  1.06it/s]
W0424 19:29:32.879323 139679895955264 logging.py:295] The dtype of attention mask (torch.int64) is not bool
2023-04-24 19:29:35.808285: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-24 19:29:35.874118: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-24 19:29:36.520157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
--------------------------------------------------------------------------
WARNING: No preset parameters were found for the device that Open MPI
detected:

  Local host:            192-9-147-106
  Device name:           mlx5_0
  Device vendor ID:      0x02c9
  Device vendor part ID: 4126

Default device parameters will be used, which may result in lower
performance.  You can edit any of the files specified by the
btl_openib_device_param_files MCA parameter to set values for your
device.

NOTE: You can turn off this warning by setting the MCA parameter
      btl_openib_warn_no_device_params_found to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           192-9-147-106
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       udcm
--------------------------------------------------------------------------
2023-04-24 19:29:37.155683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-04-24 19:29:37.157175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-04-24 19:29:37.157743: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0424 19:29:37.507889 139679895955264 utils.py:129] Note: NumExpr detected 30 cores but &quot;NUMEXPR_MAX_THREADS&quot; not set, so enforcing safe limit of 8.
I0424 19:29:37.508062 139679895955264 utils.py:141] NumExpr defaulting to 8 threads.
/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version &apos;2.7.3&apos; or newer of &apos;numexpr&apos; (version &apos;2.7.1&apos; currently installed).
  from pandas.core.computation.check import NUMEXPR_INSTALLED
ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©äººæ„Ÿåˆ°å›°æ‰°å’Œä¸å®‰ï¼Œä½†æœ‰ä¸€äº›æ–¹æ³•å¯ä»¥å¸®åŠ©ä½ å…¥ç¡ï¼š

1. åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒï¼šç¡®ä¿æˆ¿é—´å®‰é™ã€é»‘æš—ã€å‡‰çˆ½å’Œèˆ’é€‚ã€‚ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨èˆ’é€‚çš„åºŠå«å’Œæ•å¤´ï¼Œä»¥åŠä¸€ä¸ªæŸ”è½¯çš„åºŠå•å’Œè¢«å­ã€‚

2. æ”¾æ¾èº«ä½“å’Œå¤´è„‘ï¼šä½¿ç”¨æ·±å‘¼å¸ã€æ¸è¿›æ€§è‚Œè‚‰æ¾å¼›å’Œå…¶ä»–æ”¾æ¾æŠ€å·§æ¥å‡è½»èº«ä½“å’Œå¤´è„‘çš„å‹åŠ›ã€‚è¿˜å¯ä»¥è¯•ç€è¿›è¡Œå†¥æƒ³æˆ–ç‘œä¼½ç»ƒä¹ æ¥æ”¾æ¾èº«å¿ƒã€‚

3. é¿å…ä½¿ç”¨ç”µå­è®¾å¤‡ï¼šé¿å…åœ¨ç¡å‰ä½¿ç”¨ç”µå­è®¾å¤‡ï¼Œå¦‚æ‰‹æœºã€ç”µè„‘å’Œç”µè§†ã€‚è¿™äº›è®¾å¤‡å‘å‡ºçš„è“å…‰å¯èƒ½ä¼šå¹²æ‰°ç¡çœ ã€‚

4. åˆ¶å®šå›ºå®šçš„ç¡çœ æ—¶é—´è¡¨ï¼šå°½é‡åœ¨æ¯å¤©çš„ç›¸åŒæ—¶é—´ä¸ŠåºŠç¡è§‰ï¼Œå¹¶åˆ¶å®šå›ºå®šçš„èµ·åºŠæ—¶é—´ã€‚è¿™æœ‰åŠ©äºèº«ä½“å»ºç«‹å¥åº·çš„ç¡çœ ä¹ æƒ¯ã€‚

5. é¿å…æ‘„å…¥å’–å•¡å› å’Œé…’ç²¾ï¼šå’–å•¡å› å’Œé…’ç²¾å¯èƒ½ä¼šå½±å“ç¡çœ ï¼Œå› æ­¤åœ¨æ™šä¸Šå°½é‡é¿å…æ‘„å…¥è¿™äº›ç‰©è´¨ã€‚

6. å°è¯•ä¸€äº›æ”¾æ¾çš„æ´»åŠ¨ï¼šè¯•ç€åšä¸€äº›è½»æ¾çš„äº‹æƒ…ï¼Œå¦‚é˜…è¯»ã€å¬éŸ³ä¹æˆ–æ´—æ¾¡ï¼Œæ¥ç¼“è§£å‹åŠ›å’Œæ”¾æ¾èº«å¿ƒã€‚

å¦‚æœè¿™äº›æ–¹æ³•æ— æ³•å¸®åŠ©ä½ å…¥ç¡ï¼Œä½ å¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶ï¼Œè·å–æ›´ä¸“ä¸šçš„å»ºè®®å’Œå¸®åŠ©ã€‚
[INFO] GLM
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input&apos;s `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50007 for open-end generation.
[CLS] å‡¯æ—‹é—¨ä½äºæ„å¤§åˆ©ç±³å…°å¸‚å¤åŸå ¡æ—ã€‚1807å¹´ä¸ºçºªå¿µ [MASK] è€Œå»º,é—¨é«˜25ç±³,é¡¶ä¸ŠçŸ—ç«‹ä¸¤æ­¦å£«é’é“œå¤å…µè½¦é“¸åƒã€‚ &lt;|endoftext|&gt; &lt;|startofpiece|&gt; æ‹¿ç ´ä»‘å†›é˜Ÿæ”»å…‹ç±³å…°åŸ &lt;|endofpiece|&gt;
<font color="#4E9A06"><b>ubuntu@192-9-147-106</b></font>:<font color="#3465A4"><b>~/PycharmProjects/WuDao-GLM</b></font>$

</pre>
</body>
</html>
